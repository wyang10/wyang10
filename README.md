## Hi there ğŸ‘‹ I am Audrey, a Data Engineer.

<!--
**wyang10/wyang10** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ğŸ”­ Iâ€™m currently working on ...
- ğŸŒ± Iâ€™m currently learning ...
- ğŸ‘¯ Iâ€™m looking to collaborate on ...
- ğŸ¤” Iâ€™m looking for help with ...
- ğŸ’¬ Ask me about ...
- ğŸ“« How to reach me: ...
- ğŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->

# About Me

I'm a Data Engineer specializing in building scalable, cost-efficient Lakehouse architectures and production-grade ELT pipelines. I design reliable batch & streaming systems that turn messy data into trusted analytics and ML-ready datasets.

---

## Quick pitch
- ğŸ”­ Graduated: MSCS at Northeastern University (Jan 2022 â€“ Dec 2024)  
- ğŸŒ± Learning: advanced streaming patterns, cloud-native ETL optimizations  
- ğŸ’¡ Focus: Airflow + dbt orchestration, Spark (PySpark) performance, data quality with Great Expectations  
- ğŸ“« Contact: wyang10 (GitHub) â€¢ www.linkedin.com/in/awhy

---

## Highlights
- Built a production ELT & Data Quality Framework combining Airflow, dbt, and Great Expectations for automated, auditable orchestration.
- Hands-on with cloud data platforms across AWS & GCP â€” designed solutions balancing reliability, latency, and cost.
- Experienced in both batch and streaming ETL using Spark, Kafka, and Dataflow/EMR.

---

## Experience
- Data Engineer â€” LumiereX (Jan 2025 â€“ Present)  
  Led core data platform initiatives, implemented ELT frameworks, and optimized Spark workloads for cost and speed.

- Software Engineer Intern â€” VisionX (Jan 2024 â€“ Jul 2024)  
  Implemented scalable data ingestion pipelines and contributed to ML feature platforms.

---

## Featured Projects

- ![airflow_dbt_damo](https://github.com/wyang10/airflow_dbt_demo.git) 
  â€” ELT & Data Quality Framework combining Airflow + dbt + Great Expectations, production-ready orchestration for reproducible, turning manual workflows into an automated, auditable orchestration system.

- ![Macro-Market-Intelligence-Pipeline](https://github.com/wyang10/Macro-Market-Intelligence-Pipeline.git)
  â€” Automates the process of collecting, analyzing, and summarizing macroeconomic signals from multiple data sources â€” including EDGAR filings, and public macro indicators (VIX, DXY, UST10Y).It produces a weekly Markdown + HTML report featuring an AI-generated â€œExecutive Summary 2.0,â€ which compresses market data into one-line macro insights.

---

## How I work / What I enjoy
- Designing modular, testable pipelines that are easy to operate and debug.
- Choosing trade-offs that optimize for team velocity, observability, and cloud spend.
- Mentoring teammates on data modeling, pipeline testing, and operational best practices.

---

## Core Skills

Languages & Tools
- Python (Pandas, PySpark), SQL, Java, Scala, Bash

Cloud & Orchestration
- GCP: BigQuery, Dataflow
- AWS: S3, EMR, Glue, Lambda, Step Functions, IAM
- Orchestration: Airflow, dbt, Docker, GitHub Actions, Kubernetes

Big Data & Storage
- Spark, Flink, Kafka, Hive, HDFS, Databricks
- Delta Lake, Snowflake, Parquet, star-schema modeling

Data Quality & CI
- Great Expectations, dbt tests, automated lineage & monitoring

---
